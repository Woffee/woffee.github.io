<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://woffee.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://woffee.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-13T00:46:18+00:00</updated><id>https://woffee.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Geometric Explanation of Sora’s Physical Paradoxes</title><link href="https://woffee.github.io/blog/2024/Sora/" rel="alternate" type="text/html" title="A Geometric Explanation of Sora’s Physical Paradoxes"/><published>2024-02-27T13:56:00+00:00</published><updated>2024-02-27T13:56:00+00:00</updated><id>https://woffee.github.io/blog/2024/Sora</id><content type="html" xml:base="https://woffee.github.io/blog/2024/Sora/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sora/p0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sora/p0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sora/p0-1400.webp"/> <img src="/assets/img/sora/p0.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image for relax. </div> <p>At the beginning of 2024, Sora emerged out of nowhere, shocking the world. Sora boldly claims to be “a video generation model that simulates the world.” Some pessimistically predict that many traditional fields might be upended, with computer graphics, short videos, and film entertainment being among the most vulnerable. As OpenAI has revealed more technical details, many videos generated by Sora that exhibit physical paradoxes have spread online.</p> <p>Here, I offer an explanation of the current technical shortcomings in Sora’s approach based on perspectives from modern mathematics, particularly from the field of global differential geometry. My hope is to offer a modest contribution that sparks further ideas, thereby broadening the thinking of AI researchers and engineers and promoting further advancements. In this explanation, I mainly use manifold embedding theory, catastrophe theory (critical state theory), characteristic class theory of fiber bundles, the heat diffusion equation, and the regularity theory of optimal transport equations (Monge–Ampère equation).</p> <h1 id="the-manifold-distribution-principle">The Manifold Distribution Principle</h1> <p>In the field of deep learning, a natural dataset is regarded as a probability distribution on a manifold. This is known as the manifold distribution principle. We regard an observed sample as a point in the original data space. A large number of samples form a dense point cloud in the original data space that lies near a certain low-dimensional manifold; this manifold is called the data manifold. The distribution of the point cloud on the data manifold is not uniform but follows specific distribution rules, which are represented as a data probability distribution.</p> <p>Naturally, we then ask the following questions: 1. Why is the data point cloud low-dimensional rather than filling the entire original data space? 2. Why is the collection of points a manifold, i.e., locally continuous and smooth?</p> <p>The answer to the first question is: because natural phenomena adhere to numerous natural laws, these constraints reduce the dimensionality of the data sample point cloud, making it impossible to fill the entire space. For example, consider the dataset consisting of all natural human face photos. Each sample is an image, and the number of pixels multiplied by 3 gives the dimension of the original image space. Any point in the original image space represents an image, but only very few images are human face images that fall on the face image manifold. Therefore, the face image manifold cannot fill the entire original image space.</p> <p>Human faces need to satisfy many natural physiological rules. Each rule reduces the dimensionality of the data manifold. For example, bilateral symmetry almost halves the number of independent pixels; the presence of five facial features with fixed geometric and textural regions, and the similar shapes of these features with few parameters, further reduces the dimensionality. Ultimately, the genes controlling human faces are very limited, so the dimensionality of the face image manifold is far lower than the number of image pixels.</p> <p>Similarly, consider the steady-state temperature distribution over a planar region. According to the physical heat diffusion theorem, a stable function satisfies the classical Laplace equation and is uniquely determined by its boundary values. If we have ￼ sampling points inside the region and ￼ sampling points on the boundary, then each observed temperature function is represented as a vector of dimension ￼, i.e., the original data space has dimension ￼; however, the actual manifold’s dimension is that of the boundary function, which is ￼. This shows that the data manifold formed by observation samples that satisfy physical laws is far lower in dimensionality than the original data space.</p> <p>The answer to the second question is: in most cases, physical systems are well-posed, but in critical states, physical systems undergo sudden changes (described by catastrophe theory or critical state theory). Physical laws are mostly described by systems of partial differential equations. The solutions of these equations are controlled by the initial and boundary values. The system being well-posed means that, due to physical constraints such as energy conservation, mass conservation, and the speed limit of energy transfer (less than the speed of light), the solution changes gradually as the initial and boundary values change gradually. In the regularity theory of partial differential equations, this means that the Sobolev norm of the boundary values controls the Sobolev norm of the solution. We can regard the solution as a point on the data manifold and the boundary values as its corresponding local coordinates (that is, the corresponding latent feature vector in the latent space).</p> <p>The mapping from the data manifold to the latent space is called the encoding map, and the mapping from the latent space to the data manifold is called the decoding map. The regularity theory ensures that both the encoding and decoding maps are continuous, even smooth, and the uniqueness of the solution ensures that these maps are topologically or differentiably equivalent. The boundary values can be arbitrarily perturbed locally, meaning that the latent variables have an open Euclidean disc neighborhood. This implies that the observed samples that satisfy specific physical laws form a data manifold.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sora/p1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sora/p1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sora/p1-1400.webp"/> <img src="/assets/img/sora/p1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 1. Sora encodes videos into the latent space, then segments them into spatiotemporal patches, which are called time-space tokens. (openai.com) </div> <p>As shown in Fig. 1, Sora’s training set is composed of short videos, with each sample being a short video. Similar short videos form a data manifold. Sora encodes them into a latent space for dimensionality reduction, and then in the latent space, it segments the latent feature vectors into patches, which, with the addition of temporal order, form spatiotemporal patches, or time-space tokens. The concept of spatiotemporal information is critical here, as each token records both the temporal frame number (time) and the spatial row and column indices (space) of the current frame.</p> <h1 id="transformation-of-probability-distributions">Transformation of Probability Distributions</h1> <p>We can further ask the following question:</p> <ol> <li>How can the probability distribution on the data manifold be represented?</li> </ol> <p>The answer to the third question is: by using transport maps to transform the data probability distribution into a Gaussian distribution that a computer can generate. This transport map can be applied in either the original data space or the latent space. Common transport transforms include the optimal transport transform and heat diffusion. We can explain this from the perspective of fluid dynamics. Imagine the entire latent space is a water tank filled with a certain solvent, with its density representing the probability density. We disturb the water tank, causing the fluid to flow so that the density of the solvent changes. We compute the flow direction and speed of each water molecule so that the entropy of the probability density continually increases, eventually resulting in a Gaussian distribution.</p> <p>For example, consider the distribution of face images, where each water molecule corresponds to a face image. By continuously adding noise to the face images, we obtain a series of images until they become pure white noise. This series represents the trajectories of the water molecules. Eventually, each face image turns into white noise, and all these white noise distributions satisfy a Gaussian distribution. This process is known as Langevin dynamics.</p> <p>Conversely, given a white noise image, if we trace the water molecule trajectories back to their source, we recover a face image. This is the principle behind diffusion models. Alternatively, one can directly solve for a homeomorphism from the latent space to itself using optimal transport theory, transforming the data distribution into a Gaussian distribution, which requires solving the Monge–Ampère equation. Thus, all the information of the data distribution is contained in the transport map, which is expressed by a deep network.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sora/p2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sora/p2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sora/p2-1400.webp"/> <img src="/assets/img/sora/p2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 2. Sora uses a diffusion model to generate data spatiotemporal tokens from white noise spatiotemporal tokens. (openai.com) </div> <p>As shown in Fig. 2, in the latent space Sora transforms the probability distribution of the data tokens into a Gaussian distribution via a diffusion process (Langevin dynamics—gradually adding noise to each token), and then uses the inverse transform to convert the white noise tokens in the latent space back into data tokens.</p> <h1 id="the-enhancement-by-large-language-models">The Enhancement by Large Language Models</h1> <p>Sora integrates the large language model ChatGPT, which greatly improves the performance of the system. Firstly, Sora’s training samples are (text, video) pairs. Some videos have titles that are too brief or lack subtitles, so Sora employs DALL-E’s re-titling technique.</p> <p>Sora’s training set includes some high-quality samples (highly descriptive subtitles and short videos), which are used to train the data manifold of short videos (including the spatiotemporal token manifold), with each manifold being identified by its subtitles (titles). For poor-quality short videos that lack titles or have ambiguous subtitles, Sora encodes them into the latent space and searches for latent feature vectors that are close to those of high-quality videos, then copies the subtitles (titles) from the high-quality videos to the poor ones. In this way, Sora can add highly descriptive subtitles to all the training video data, thereby improving the quality of the training set and further enhancing system performance.</p> <p>At the same time, the large language model can expand the user’s input prompts, making them more precise and descriptive, so that the generated videos better match the user’s needs. This gives Sora an extra boost. However, Sora still has many shortcomings, which we can analyze through the following examples.</p> <h1 id="the-contradiction-between-correlation-and-causality">The Contradiction Between Correlation and Causality</h1> <p>ChatGPT breaks down sentences into tokens and then uses a Transformer to learn the probability distribution of connections among tokens in context. Similarly, Sora breaks down videos into spatiotemporal tokens and then learns the probability distribution of connections among tokens in context. Based on this probability distribution, it generates tokens from white noise, connects them, and decodes them into short videos.</p> <p>Each token represents a local region in an image or video, and the stitching together of different local regions becomes the key issue. Sora learns each token relatively independently, expressing the spatial relationships between tokens using probabilities derived from the training set. As a result, it is unable to precisely express the spatiotemporal causal relationships between tokens.</p> <p>Video 1. Video of an elderly lady blowing out birthday candles generated by Sora. (openai.com)</p> <p>As shown in Video 1, every frame in the video generated by Sora is extraordinarily realistic, yet when the elderly lady blows out the birthday candles, the flame remains completely still. If we zoom into the region corresponding to each token, we see a gorgeous, realistic picture with very smooth and natural transitions between tokens. However, when there is a causal relationship between tokens that are far apart—for example, the air blown affecting the flickering of the flame—the physical causality between the two tokens is not reflected.</p> <p>This means that a Transformer, which is used to express the statistical correlations between tokens, cannot precisely express physical causality. Although Transformers can manipulate natural language to some extent, natural language cannot accurately express physical laws, which are currently expressed precisely only by partial differential equations. This reflects a certain limitation of world models based on probability.</p> <h1 id="the-contradiction-between-local-plausibility-and-global-absurdity">The Contradiction Between Local Plausibility and Global Absurdity</h1> <p>Currently, Sora does a reasonable job stitching together adjacent tokens, but the overall assembled video may exhibit various paradoxes. This indicates a gap between local stitching and global expansion.</p> <p>Video 2. The “Ghost Chair” video generated by Sora. (openai.com) If we observe the “Ghost Chair” video and limit our view to a local area in the center of the screen, the video appears entirely reasonable. A careful examination of the transitions between different token regions shows very continuous and smooth connections. However, the entire chair appears to be mysteriously suspended in mid-air, which contradicts everyday experience.</p> <p>This kind of “locally plausible, globally absurd” video generation suggests that the Transformer has learned the local connection probabilities between tokens but lacks a broader, global understanding of the spatiotemporal context. In this video, the global concept comes from the gravitational field in physics, which is omnipresent though not apparent locally.</p> <p>Video 3. The “Quadruped Ant” video generated by Sora. (openai.com)</p> <p>Another example is the video of “quadruped ants” generated by Sora. The ants move vividly, almost like flowing clouds. Locally, the movements are very smooth and natural, making one wonder if such quadruped ants might exist on some planet. However, globally, there are no quadruped ants in the natural world on Earth. Here, local plausibility does not guarantee global plausibility, as the global perspective comes from biological facts.</p> <p>Video 4. The “Contradictory Treadmill” video generated by Sora. (openai.com)</p> <p>Similarly, in Sora’s “Contradictory Treadmill” video, if we examine each local region, the images appear reasonable, and the connections between tokens seem natural. However, the overall video is absurd: the treadmill moves in the opposite direction to the runner. This global perspective contradicts the facts of ergonomics.</p> <p>These examples indicate that although the current Transformer can learn local contexts, it is unable to learn a more global context. This global context may be the gravitational field in physics, ergonomics, or biological species classification. This global perspective is precisely what Professor Zhu Songchun described as the “dark matter” of the AI world. Although each training sample video implicitly expresses a global perspective, the tokenization process fragments this global view, retaining only the limited connection probabilities between neighboring tokens, leading to locally plausible yet globally absurd results.</p> <p>Modern global differential geometry places great emphasis on the contradiction between the local and the global, and has thus invented various theoretical tools. For instance, one can construct smooth frame fields locally on a topological manifold, but these cannot be globally extended due to the obstruction given by the characteristic classes of fiber bundles. On complex manifolds, one can locally construct meromorphic functions, but globally these local functions cannot be pieced together into a global meromorphic function. The discrepancy between local extension and global existence can be precisely characterized by the sheaf cohomology theory.</p> <p>Many physical theories are expressed in terms of the characteristic class theories of specific fiber bundles, such as the theory of topological insulators. This type of mathematical theory, which is easy to construct locally but encounters substantial difficulties when extended globally, is in fact a crystallization of humanity’s profound exploration of nature. Such global topological and geometrical perspectives have not yet been extended into the AI field. If Transformers could learn these global obstructions in context on their own, AI would be much more effective in exploring the natural world.</p> <h1 id="the-absence-of-critical-states">The Absence of Critical States</h1> <p>The vast majority of physical processes in nature alternate between steady states and critical states. In steady states, the system parameters change slowly, making it easy to obtain observational data; in critical states (catastrophic changes), the system suddenly shifts, catching one off guard, making it very hard to capture observational data. Consequently, critical state samples are very scarce and almost of zero measure in the training set.</p> <p>As a result, the data manifold learned by the Sora system is almost entirely composed of samples from steady states. In physical processes, critical state samples are mostly distributed along the boundaries of the data manifold. Therefore, during the generation process, Sora very easily generates video segments corresponding to steady states, but it often skips the critical states. Yet in human perception, the most crucial observations are precisely those critical states that occur with almost zero probability.</p> <p>Video 5. Video of juice splashing generated by Sora. (openai.com) In the juice splashing video generated by Sora, there are two stable states: the state in which the cup is upright and the state in which the juice has already splashed out. However, the most critical state—the process of the juice spilling from the cup—is not generated. Although it only lasts a few frames, this process is extremely important for human perception of the entire event. Sora’s failure to generate images of the critical state may be due to the following reasons:</p> <p>Different steady state samples in a physical process generate different connected branches of the data manifold, and the critical state samples lie near the boundary of the steady state manifold, between the boundaries of two steady state manifolds. The thermodynamic diffusion process blurs the boundaries of the manifold, thus confusing the manifold boundaries and generating videos with ambiguous transitions. In other words, the near-critical state corresponds to the boundary of the data manifold, and during learning, the boundary conditions should be preserved rather than causing mode confusion.</p> <p>As shown in Fig. 3, we trained an encoder-decoder using MNIST and plotted the latent space distribution of the dataset. The 10 handwritten digits correspond to 10 clusters, each cluster representing a mode, i.e., a connected branch of the data manifold. The boundaries of the clusters are the boundaries of the support of the data’s latent space distribution. We generated 100 sample points in the latent space and decoded them into 100 images of handwritten digits. If a sample point falls well within a cluster, the generated image is very clear; if it falls outside in the boundary region of a cluster, the generated image is very blurry, often a fusion of two handwritten digits. Therefore, identifying the boundaries of the data manifold is very important for recognizing critical states.</p> <p>The popular diffusion model currently employed by Sora inevitably smooths the boundaries of the steady state data manifold when computing the transport map, thus confusing different modes and directly skipping the generation of critical state images. Consequently, the video appears to abruptly jump from one state to another, with the most critical transitional process missing, leading to a physical absurdity.</p> <p>Video 6. Video of puppies generated by Sora. (openai.com)</p> <p>Video 6 illustrates another scenario where errors occur due to crossing the manifold boundaries. In the video, a group of puppies is frolicking, sometimes blocking each other, sometimes scattering apart. At one moment in the video, three puppies in the frame suddenly become four. Our explanation is as follows: images of four puppies constitute one manifold (or connected branch), while images of three puppies constitute another branch. At the boundary of the four-puppy image manifold, there is a critical event: when the four puppies overlap, only three are visible in the image.</p> <p>Sora’s diffusion model does not recognize the boundaries of the manifold; instead, it crosses these boundaries, jumping between the three-puppy image manifold and the four-puppy image manifold. The correct approach should be to first identify the boundaries of the manifold, and then, in situations where physical crossing is impossible (such as three versus four), to reflect back onto the original manifold at the boundary.</p> <p>Fig. 4. The optimal transport map based on geometric methods can precisely detect the boundaries of the data manifold and accurately capture the critical states.</p> <p>The shortcomings of the diffusion model can be overcome by the optimal transport model based on geometric methods. As shown in Fig. 4, suppose we compute the optimal transport map from a uniform distribution inside a disc to a uniform distribution within a hippocampal-shaped region on the right. According to the Brenier theorem, the optimal transport map is given by the gradient of a convex potential function. This potential function satisfies the Monge–Ampère equation. The potential function is not differentiable everywhere; the set where it is continuous but not differentiable projects onto the singular set (the black curve) in the disc. Regular points map to regular points in the target region, while the singular set maps to the boundary of the target region (each singular point mapping simultaneously to two boundary points on the left and right).</p> <p>When we cross the singular set, it means we have crossed between two steady states, and a critical (catastrophic) event must occur, namely a physical event where the steady state is broken. Thus, precisely identifying the singular set of the transport map and detecting the critical (catastrophic) states is fundamentally important for modeling the physical world.</p> <h1 id="summary">Summary</h1> <p>In summary, although Sora claims to be “a video generation model that simulates the world,” its current technical approach cannot correctly simulate the physical laws of the world.</p> <p>Firstly, using statistical correlations based on probability cannot precisely express the causality of physical laws; the contextual correlations of natural language do not reach the precision of partial differential equations. Secondly, although Transformers can learn the connection probabilities between adjacent spatiotemporal tokens, they cannot judge global plausibility. Global plausibility requires higher-level mathematical theoretical perspectives or deeper, more implicit backgrounds in natural and human sciences—perspectives that current Transformers are incapable of truly grasping.</p> <p>In addition, Sora neglects the most critical aspect of physical processes: the critical (catastrophic) state. On one hand, this is because critical state samples are scarce; on the other hand, the diffusion model blurs the boundaries of the steady state data manifold, eliminating the existence of critical states and causing jumps between different steady states in the generated videos.</p> <p>In contrast, the optimal transport theory framework based on geometric methods can precisely detect the boundaries of the steady state data manifold, thereby emphasizing the generation of critical state events and avoiding crossovers between different steady states, thus being closer to physical reality.</p> <p>Currently, the data-driven world simulation models represented by Sora and the world simulation models based on first-principles physical laws and partial differential equations have entered into a fierce battle. This may be a great turning point in human history. I hope that young readers will actively join the torrent of our times and use their intelligence and talent to promote the development of technology and society!</p> <h2 id="reference">Reference</h2> <ul> <li>Gu Xianfeng - A Geometric Explanation of Sora’s Physical Paradoxes (顾险峰 - Sora物理悖谬的几何解释)</li> </ul>]]></content><author><name></name></author><category term="PHD"/><category term="DL"/><category term="NLP"/><category term="CV"/><summary type="html"><![CDATA[At the beginning of 2024, Sora emerged out of nowhere, shocking the world. Sora boldly claims to be “a video generation model that simulates the world.” Some pessimistically predict that many traditional fields might be upended, with computer graphics, short videos, and film entertainment being among the most vulnerable. As OpenAI has revealed more technical details, many videos generated by Sora that exhibit physical paradoxes have spread online.]]></summary></entry><entry><title type="html">VR/AR Changes Life</title><link href="https://woffee.github.io/blog/2024/VR-AR-Changes-Life/" rel="alternate" type="text/html" title="VR/AR Changes Life"/><published>2024-02-02T12:56:00+00:00</published><updated>2024-02-02T12:56:00+00:00</updated><id>https://woffee.github.io/blog/2024/VR-AR-Changes-Life</id><content type="html" xml:base="https://woffee.github.io/blog/2024/VR-AR-Changes-Life/"><![CDATA[<p>By adding another dimension, virtual reality makes even household chores fun.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://pbs.twimg.com/media/GFV2qU4WUAAmOSX?format=jpg&amp;name=large-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://pbs.twimg.com/media/GFV2qU4WUAAmOSX?format=jpg&amp;name=large-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://pbs.twimg.com/media/GFV2qU4WUAAmOSX?format=jpg&amp;name=large-1400.webp"/> <img src="https://pbs.twimg.com/media/GFV2qU4WUAAmOSX?format=jpg&amp;name=large" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Household chores. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://pbs.twimg.com/media/GFV2rlfXsAACywx?format=jpg&amp;name=large-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://pbs.twimg.com/media/GFV2rlfXsAACywx?format=jpg&amp;name=large-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://pbs.twimg.com/media/GFV2rlfXsAACywx?format=jpg&amp;name=large-1400.webp"/> <img src="https://pbs.twimg.com/media/GFV2rlfXsAACywx?format=jpg&amp;name=large" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Playing piano (1). </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://pbs.twimg.com/media/GFV2pEKX0AA3tGa?format=jpg&amp;name=large-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://pbs.twimg.com/media/GFV2pEKX0AA3tGa?format=jpg&amp;name=large-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://pbs.twimg.com/media/GFV2pEKX0AA3tGa?format=jpg&amp;name=large-1400.webp"/> <img src="https://pbs.twimg.com/media/GFV2pEKX0AA3tGa?format=jpg&amp;name=large" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Playing piano (2). </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://pbs.twimg.com/media/GFV2nYnWYAAW90g?format=jpg&amp;name=medium-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://pbs.twimg.com/media/GFV2nYnWYAAW90g?format=jpg&amp;name=medium-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://pbs.twimg.com/media/GFV2nYnWYAAW90g?format=jpg&amp;name=medium-1400.webp"/> <img src="https://pbs.twimg.com/media/GFV2nYnWYAAW90g?format=jpg&amp;name=medium" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Games. </div>]]></content><author><name></name></author><category term="Life"/><category term="Life"/><summary type="html"><![CDATA[By adding another dimension, virtual reality makes even household chores fun.]]></summary></entry><entry><title type="html">Things that strengthen an IR paper</title><link href="https://woffee.github.io/blog/2024/IR-paper-recommendations/" rel="alternate" type="text/html" title="Things that strengthen an IR paper"/><published>2024-02-02T12:56:00+00:00</published><updated>2024-02-02T12:56:00+00:00</updated><id>https://woffee.github.io/blog/2024/IR-paper-recommendations</id><content type="html" xml:base="https://woffee.github.io/blog/2024/IR-paper-recommendations/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://pbs.twimg.com/media/GEnzBLLWEAE8jJg?format=jpg&amp;name=4096x4096-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://pbs.twimg.com/media/GEnzBLLWEAE8jJg?format=jpg&amp;name=4096x4096-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://pbs.twimg.com/media/GEnzBLLWEAE8jJg?format=jpg&amp;name=4096x4096-1400.webp"/> <img src="https://pbs.twimg.com/media/GEnzBLLWEAE8jJg?format=jpg&amp;name=4096x4096" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image for relax. </div> <h2 id="presentation">Presentation</h2> <ol> <li>The paper’s motivation and the potential impact of the addressed problem are discussed.</li> <li>The paper’s original contributions (i.e. the delta over prior art) are clearly stated.</li> <li>The paper’s claims are properly scoped and supported.</li> <li>The paper clearly describes what was done and what was not.</li> <li>The choices made in each step of the research are justified (the why’s).</li> <li>The results are presented effectively in appropriate format.</li> <li>Good discussion accompanies the results.</li> </ol> <h2 id="experimentation-if-applicable">Experimentation (if applicable)</h2> <ol> <li>The experimental design and its scale are appropriate.</li> <li>In comparative studies, appropriate baselines are used.</li> <li>The experimental results are reliable and generalizable.</li> <li>The evaluation methods employed are in line with the research questions.</li> <li>Statistical analysis is performed and reported appropriately.</li> <li>Sufficient details (with data and code where appropriate) are provided to help other researchers assess and reproduce the experiments.</li> </ol>]]></content><author><name></name></author><category term="PHD"/><category term="PHD"/><summary type="html"><![CDATA[Things that strengthen an IR paper, recommendations from the Program Chairs.]]></summary></entry><entry><title type="html">Using joblib.Memory to cache function results</title><link href="https://woffee.github.io/blog/2023/Memory-Cache/" rel="alternate" type="text/html" title="Using joblib.Memory to cache function results"/><published>2023-10-22T12:56:00+00:00</published><updated>2023-10-22T12:56:00+00:00</updated><id>https://woffee.github.io/blog/2023/Memory-Cache</id><content type="html" xml:base="https://woffee.github.io/blog/2023/Memory-Cache/"><![CDATA[<p>If we need to call our function several time with the same input data, it is beneficial to avoid recomputing the same results over and over since it is expensive. <code class="language-plaintext highlighter-rouge">joblib.Memory</code> enables to cache results from a function into a specific location.</p> <h3 id="example">Example</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">joblib</span> <span class="kn">import</span> <span class="n">Memory</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">location</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./cachedir</span><span class="sh">'</span>
<span class="n">memory</span> <span class="o">=</span> <span class="nc">Memory</span><span class="p">(</span><span class="n">location</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>


<span class="nd">@memory.cache</span>
<span class="k">def</span> <span class="nf">my_costly_compute_cached</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">column_index</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Simulate an expensive computation</span><span class="sh">"""</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="n">column_index</span><span class="p">]</span>



<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">data_trans</span> <span class="o">=</span> <span class="nf">my_costly_compute_cached</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">The function took {:.2f} s to compute.</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">The transformed data are:</span><span class="se">\n</span><span class="s"> {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">data_trans</span><span class="p">))</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">data_trans</span> <span class="o">=</span> <span class="nf">my_costly_compute_cached</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">The function took {:.2f} s to compute.</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">The transformed data are:</span><span class="se">\n</span><span class="s"> {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">data_trans</span><span class="p">))</span>
</code></pre></div></div> <h3 id="outputs">Outputs</h3> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The function took 5.08 s to compute.

The transformed data are:
 [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696
  1.57921282  0.76743473 -0.46947439  0.54256004]

The function took 0.03 s to compute.

The transformed data are:
 [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696
  1.57921282  0.76743473 -0.46947439  0.54256004]
</code></pre></div></div> <h3 id="references">References</h3> <ul> <li><a href="https://joblib.readthedocs.io/en/latest/auto_examples/memory_basic_usage.html">https://joblib.readthedocs.io/en/latest/auto_examples/memory_basic_usage.html</a></li> </ul>]]></content><author><name></name></author><category term="PHD"/><category term="Code"/><summary type="html"><![CDATA[This example illustrates the usage of joblib.Memory with functions.]]></summary></entry><entry><title type="html">K-Fold Cross Validation</title><link href="https://woffee.github.io/blog/2023/K-Fold-Cross-Validation/" rel="alternate" type="text/html" title="K-Fold Cross Validation"/><published>2023-08-25T13:56:00+00:00</published><updated>2023-08-25T13:56:00+00:00</updated><id>https://woffee.github.io/blog/2023/K-Fold-Cross-Validation</id><content type="html" xml:base="https://woffee.github.io/blog/2023/K-Fold-Cross-Validation/"><![CDATA[<p>Source: <a href="https://www.analyticsvidhya.com/blog/2022/02/k-fold-cross-validation-technique-and-its-essentials/#h-what-is-k-fold-cross-validation">K-Fold Cross Validation Technique and its Essentials</a></p> <h2 id="what-is-k-fold-cross-validation">What is K-Fold Cross Validation?</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/K-fold-cross-vaslidation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/K-fold-cross-vaslidation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/K-fold-cross-vaslidation-1400.webp"/> <img src="/assets/img/K-fold-cross-vaslidation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Life Cycle of K-Fold Cross-Validation. </div> <p>K-fold cross-validation is a technique for evaluating predictive models. The dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model’s generalization performance. This method aids in model assessment, selection, and hyperparameter tuning, providing a more reliable measure of a model’s effectiveness.</p> <p>In each set (fold) training and the test would be performed precisely once during this entire process. It helps us to avoid overfitting. As we know when a model is trained using all of the data in a single short and give the best performance accuracy. To resist this k-fold cross-validation helps us to build the model is a generalized one.</p> <p>To achieve this K-Fold Cross Validation, we have to split the data set into three sets, Training, Testing, and Validation, with the challenge of the volume of the data.</p> <p>Here Test and Train data set will support building model and hyperparameter assessments.</p> <p>In which the model has been validated multiple times based on the value assigned as a parameter and which is called K and it should be an INTEGER.</p> <p>Make it simple, based on the K value, the data set would be divided, and train/testing will be conducted in a sequence way equal to K time.</p> <h2 id="life-cycle-of-k-fold-cross-validation">Life Cycle of K-Fold Cross-Validation</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/K-fold-cross-vaslidation2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/K-fold-cross-vaslidation2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/K-fold-cross-vaslidation2-1400.webp"/> <img src="/assets/img/K-fold-cross-vaslidation2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Life Cycle of K-Fold Cross-Validation. </div> <p>Let’s have a generalised K value. If K=5, it means, in the given dataset and we are splitting into 5 folds and running the Train and Test. During each run, one fold is considered for testing and the rest will be for training and moving on with iterations, the below pictorial representation would give you an idea of the flow of the fold-defined size.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/K-fold-cross-vaslidation3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/K-fold-cross-vaslidation3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/K-fold-cross-vaslidation3-1400.webp"/> <img src="/assets/img/K-fold-cross-vaslidation3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Life Cycle of K-Fold Cross-Validation. </div> <p>In which each data point is used, once in the hold-out set and K-1 in Training. So, during the full iteration at least once, one fold will be used for testing and the rest for training.</p> <p>In the above set, 5- Testing 20 Training. In each iteration, we will get an accuracy score and have to sum them and find the mean. Here we can understand how the data is spread in a way of consistency and will make a conclusion whether to for the production with this model (or) NOT.</p> <h2 id="model-selection-using-k-fold">Model Selection using K-Fold</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">digits</span> <span class="o">=</span> <span class="nf">load_digits</span><span class="p">()</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">,</span><span class="n">digits</span><span class="p">.</span><span class="n">target</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div></div> <h2 id="frequently-asked-questions">Frequently Asked Questions</h2> <h3 id="q1-what-is-k-fold-cross-validation-value">Q1. What is k-fold cross-validation value?</h3> <p>A. K-fold cross-validation is a technique used in machine learning and statistical modeling to evaluate the performance of a predictive model. It involves dividing the dataset into k subsets or folds of approximately equal size. The model is then trained and evaluated k times, each time using a different fold as the validation set and the remaining folds as the training set. The performance metrics obtained from each fold are averaged to provide a more robust estimate of the model’s generalization performance. Common values for k are 5 and 10, but other values can also be used depending on the dataset size and complexity.</p> <h3 id="q2-how-do-you-use-k-fold-cross-validation">Q2. How do you use K-fold cross validation?</h3> <p>A. K-fold cross-validation is used to assess the performance of a machine learning model and to estimate its generalization ability. Here are the steps to utilize K-fold cross-validation:</p> <ol> <li>Split the data: Divide your dataset into k equal-sized subsets (folds). Typically, k is chosen as 5 or 10, but you can adjust it based on your needs.</li> <li>Train and validate: Iterate over the k folds. In each iteration, use k-1 folds for training the model and the remaining fold for validation. Train your model on the training folds and evaluate its performance on the validation fold.</li> <li>Performance metrics: Calculate the performance metric(s) of interest (e.g., accuracy, precision, recall) for each fold. These metrics quantify how well the model generalizes to unseen data.</li> <li>Average the results: Average the performance metrics obtained from the k folds to obtain a more robust estimate of the model’s performance. This average value represents the overall performance of the model.</li> <li>Model selection and tuning: Based on the cross-validation results, you can compare different models or hyperparameter settings and select the one that performs the best on average across the folds.</li> <li>Final evaluation: After selecting the model or hyperparameters, you can retrain the model using the entire dataset and evaluate its performance on a separate test set to obtain a final performance estimation. K-fold cross-validation helps to mitigate the risk of overfitting and provides a more reliable assessment of how well the model is expected to perform on unseen data.</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Guys! so far, we have discussed various aspects of the K-Fold Cross Validation Technique and its importance in the Machine Learning model for production, parameter selection with a classical example. I trust this article would help you all to understand this topic better. If you ask me if we have any disadvantages of this, my answer would be Yes! That is nothing but slow in execution than straightforward training and test. In spite of this small drawback, K Fold cross-validation plays a critical role in the Machine Learning world.</p>]]></content><author><name></name></author><category term="PHD"/><category term="Code"/><summary type="html"><![CDATA[K-fold cross-validation is a technique for evaluating predictive models.]]></summary></entry><entry><title type="html">How to Write the Scope of the Study</title><link href="https://woffee.github.io/blog/2023/scope-of-study/" rel="alternate" type="text/html" title="How to Write the Scope of the Study"/><published>2023-06-12T13:56:00+00:00</published><updated>2023-06-12T13:56:00+00:00</updated><id>https://woffee.github.io/blog/2023/scope-of-study</id><content type="html" xml:base="https://woffee.github.io/blog/2023/scope-of-study/"><![CDATA[<p>Source: <a href="https://www.discoverphds.com/blog/scope-of-the-study">https://www.discoverphds.com/blog/scope-of-the-study</a></p> <p>The scope of the study is defined at the start of the research project before data collection begins. It is used by researchers to set the boundaries and limitations within which the study will be performed.</p> <h2 id="what-is-the-scope-of-the-study">What is the Scope of the Study?</h2> <p>The scope of the study refers to the boundaries within which your research project will be performed; this is sometimes also called the scope of research. To define the scope of the study is to define all aspects that will be considered in your research project. It is also just as important to make clear what aspects will not be covered; i.e. what is outside of the scope of the study.</p> <h2 id="why-is-the-scope-of-the-study-important">Why is the Scope of the Study Important?</h2> <p>The scope of the study is always considered and agreed upon in the early stages of the project, before any data collection or experimental work has started. This is important because it focuses the work of the proposed study down to what is practically achievable within a given timeframe.</p> <p>A well-defined research or study scope enables a researcher to give clarity to the study outcomes that are to be investigated. It makes clear why specific data points have been collected whilst others have been excluded.</p> <p>Without this, it is difficult to define an end point for a research project since no limits have been defined on the work that could take place. Similarly, it can also make the approach to answering a research question too open ended.</p> <h2 id="how-do-you-write-the-scope-of-the-study">How do you Write the Scope of the Study?</h2> <p>In order to write the scope of the study that you plan to perform, you must be clear on the research parameters that you will and won’t consider. These parameters usually consist of the sample size, the duration, inclusion and exclusion criteria, the methodology and any geographical or monetary constraints.</p> <p>Each of these parameters will have limits placed on them so that the study can practically be performed, and the results interpreted relative to the limitations that have been defined. These parameters will also help to shape the direction of each research question you consider.</p> <p>The term limitations’ is often used together with the scope of the study to describe the constraints of any parameters that are considered and also to clarify which parameters have not been considered at all. Make sure you get the balance right here between not making the scope too broad and unachievable, and it not being too restrictive, resulting in a lack of useful data.</p> <p>The sample size is a commonly used parameter in the definition of the research scope. For example, a research project involving human participants may define at the start of the study that 100 participants will be recruited. This number will be determined based on an understanding of the difficulty in recruiting participants to studies and an agreement of an acceptable period of time in which to recruit this number.</p> <p>Any results that are obtained by the research group can then be interpreted by others with the knowledge that the study was capped to 100 participants and an acceptance of this as a limitation of the study. In other words, it is acknowledged that recruiting 100 rather than 1,000 participants has limited the amount of data that could be collected, however this is an acceptable limitation due to the known difficulties in recruiting so many participants (e.g. the significant period of time it would take and the costs associated with this).</p> <h2 id="example-of-a-scope-of-the-study">Example of a Scope of the Study</h2> <p>The follow is a (hypothetical) example of the definition of the scope of the study, with the research question investigating <em>the impact of the COVID-19 pandemic on mental health</em>.</p> <p>Whilst the immediate negative health problems related to the COVID-19 pandemic have been well documented, the impact of the virus on the mental health (MH) of young adults (age 18-24 years) is poorly understood. The aim of this study is to report on MH changes in population group due to the pandemic.</p> <p>The scope of the study is limited to recruiting 100 volunteers between the ages of 18 and 24 who will be contacted using their university email accounts. This recruitment period will last for a maximum of 2 months and will end when either 100 volunteers have been recruited or 2 months have passed. Each volunteer to the study will be asked to complete a short questionnaire in order to evaluate any changes in their MH.</p> <p>From this example we can immediately see that the scope of the study has placed a constraint on the sample size to be used and/or the time frame for recruitment of volunteers. It has also introduced a limitation by only opening recruitment to people that have university emails; i.e. anyone that does not attend university will be excluded from this study.</p> <p>This may be an important factor when interpreting the results of this study; the comparison of MH during the pandemic between those that do and do not attend university, is therefore outside the scope of the study here. We are also told that the methodology used to assess any changes in MH are via a questionnaire. This is a clear definition of how the outcome measure will be investigated and any other methods are not within the scope of research and their exclusion may be a limitation of the study.</p> <h2 id="conclusion">Conclusion</h2> <p>The scope of the study is important to define as it enables a researcher to focus their research to within achievable parameters.</p>]]></content><author><name></name></author><category term="Blog"/><category term="PHD"/><summary type="html"><![CDATA[In this post you will learn exactly what the scope of the study means, why it is important in your research, how you would write one and finally you’ll be presented with an example scope of a study.]]></summary></entry><entry><title type="html">Show labels when hovering over a point</title><link href="https://woffee.github.io/blog/2023/Matplotlib-Show-labels-when-hovering-over-a-point/" rel="alternate" type="text/html" title="Show labels when hovering over a point"/><published>2023-01-13T13:56:00+00:00</published><updated>2023-01-13T13:56:00+00:00</updated><id>https://woffee.github.io/blog/2023/Matplotlib-Show-labels-when-hovering-over-a-point</id><content type="html" xml:base="https://woffee.github.io/blog/2023/Matplotlib-Show-labels-when-hovering-over-a-point/"><![CDATA[<p>Show labels when hovering over a point.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/matplotlib-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/matplotlib-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/matplotlib-1400.webp"/> <img src="/assets/img/matplotlib.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Show labels when hovering over a point. </div> <p>This code is modified based on <a href="https://stackoverflow.com/questions/7908636/possible-to-make-labels-appear-when-hovering-over-a-point-in-matplotlib">here</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">class</span> <span class="nc">My_show</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">names</span> <span class="o">=</span> <span class="n">labels</span>

        <span class="n">self</span><span class="p">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">colors</span>

        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">RdYlGn</span>

        <span class="n">self</span><span class="p">.</span><span class="n">fig</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sc</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">c</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">cmap</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">norm</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">annot</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="sh">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="sh">"</span><span class="s">offset points</span><span class="sh">"</span><span class="p">,</span>
                            <span class="n">bbox</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="sh">"</span><span class="s">round</span><span class="sh">"</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">),</span>
                            <span class="n">arrowprops</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="sh">"</span><span class="s">-&gt;</span><span class="sh">"</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_annot</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ind</span><span class="p">):</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sc</span><span class="p">.</span><span class="nf">get_offsets</span><span class="p">()[</span><span class="n">ind</span><span class="p">[</span><span class="sh">"</span><span class="s">ind</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="n">xy</span> <span class="o">=</span> <span class="n">pos</span>
        <span class="c1"># text = "{}, {}".format(" ".join(list(map(str,ind["ind"]))),
</span>        <span class="c1">#                        " ".join([self.names[n] for n in ind["ind"]]))
</span>        <span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">names</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ind</span><span class="p">[</span><span class="sh">"</span><span class="s">ind</span><span class="sh">"</span><span class="p">]])</span>

        <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="nf">set_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="nf">get_bbox_patch</span><span class="p">().</span><span class="nf">set_facecolor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">cmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">c</span><span class="p">[</span><span class="n">ind</span><span class="p">[</span><span class="sh">"</span><span class="s">ind</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]])))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="nf">get_bbox_patch</span><span class="p">().</span><span class="nf">set_alpha</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">hover</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">event</span><span class="p">):</span>
        <span class="n">vis</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="nf">get_visible</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">event</span><span class="p">.</span><span class="n">inaxes</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">ax</span><span class="p">:</span>
            <span class="n">cont</span><span class="p">,</span> <span class="n">ind</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sc</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cont</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">update_annot</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">fig</span><span class="p">.</span><span class="n">canvas</span><span class="p">.</span><span class="nf">draw_idle</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">vis</span><span class="p">:</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">annot</span><span class="p">.</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">fig</span><span class="p">.</span><span class="n">canvas</span><span class="p">.</span><span class="nf">draw_idle</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">show</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fig</span><span class="p">.</span><span class="n">canvas</span><span class="p">.</span><span class="nf">mpl_connect</span><span class="p">(</span><span class="sh">"</span><span class="s">motion_notify_event</span><span class="sh">"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hover</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">my</span> <span class="o">=</span> <span class="nc">My_show</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">colors</span><span class="p">)</span>
    <span class="n">my</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>]]></content><author><name></name></author><category term="Blog"/><category term="Code"/><summary type="html"><![CDATA[Show labels when hovering over a point]]></summary></entry><entry><title type="html">DeepVD</title><link href="https://woffee.github.io/blog/2023/DeepVD/" rel="alternate" type="text/html" title="DeepVD"/><published>2023-01-13T13:56:00+00:00</published><updated>2023-01-13T13:56:00+00:00</updated><id>https://woffee.github.io/blog/2023/DeepVD</id><content type="html" xml:base="https://woffee.github.io/blog/2023/DeepVD/"><![CDATA[<p>The advances of machine learning (ML) including deep learning (DL) have enabled several approaches to implicitly learn vulnerable code patterns to automatically detect software vulnerabilities.</p> <h2 id="abstract">Abstract</h2> <p>A recent study showed that despite successes, the existing ML/DL-based vulnerability detection (VD) models are limited in the ability to distinguish between the two classes of vulnerability and benign code. We propose DeepVD, a graph-based neural network VD model that emphasizes on class-separation features between vulnerability and benign code. DeepVD leverages three types of class-separation features at different levels of abstraction: statement types (similar to Part-of-Speech tagging), Post-Dominator Tree (covering regular flows of execution), and Exception Flow Graph (covering the exception and error-handling flows). We conducted several experiments to evaluate DeepVD in a real-world vulnerability dataset of 303 projects with 13,130 vulnerable methods. Our results show that DeepVD relatively improves over the state-of-the-art ML/DL-based VD approaches 13%–29.6% in precision, 15.6%–28.9% in recall, and 16.4%–25.8% in F-score. Our ablation study confirms that our designed features and components help DeepVD achieve high class-separability for vulnerability and benign code. <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>https://conf.researchr.org/details/icse-2023/icse-2023-technical-track/155/DeepVD-Toward-Class-Separation-Features-for-Neural-Network-Vulnerability-Detection <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="PHD"/><category term="DL"/><category term="Publications"/><summary type="html"><![CDATA[DeepVD - Toward Class-Separation Features for Neural Network Vulnerability Detection]]></summary></entry><entry><title type="html">QA4GIS</title><link href="https://woffee.github.io/blog/2021/QA4GIS/" rel="alternate" type="text/html" title="QA4GIS"/><published>2021-07-19T13:56:00+00:00</published><updated>2021-07-19T13:56:00+00:00</updated><id>https://woffee.github.io/blog/2021/QA4GIS</id><content type="html" xml:base="https://woffee.github.io/blog/2021/QA4GIS/"><![CDATA[<p>Community-based question answering websites have attracted more and more scholars and developers to discuss domain knowledge and software development. In this article, we focus on the GIS section of the Stack Exchange website and develop a novel approach, QA4GIS, a deep learning-based system for question answering tasks with a deep neural network (DNN) model to extract the representation of the query–API document pair. We use the LambdaMART model to rerank the candidate API documents. We begin with an empirical analysis of the questions and answers, demonstrating that API documents could answer 52.93% of the questions. Then we evaluate QA4GIS by comparing it with 10 other baselines. The experiment results show that QA4GIS can improve 21.39% on the MAP score and 22.34% on the MRR score compared with the best baseline SIF. <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Wang, W., Li, Y., Wang, S., &amp; Ye, X. (2021). QA4GIS: A novel approach learning to answer GIS developer questions with API documentation. Transactions in GIS, 00, 1-26. http://doi.org/10.1111/tgis.12798 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="PHD"/><category term="DL"/><category term="NLP"/><category term="GIS"/><category term="Publications"/><summary type="html"><![CDATA[A novel approach learning to answer GIS developer questions with API documentation]]></summary></entry></feed>